{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN to Recognise Words\n",
    "\n",
    "The stamp process in the '_Data' notebook has produced some nice-looking spectrograms with a uniform (64,32) shape. \n",
    "\n",
    "Let's just recognise the words the stamps represent by learning to differentiate between the 'stamp' images : a task for which the MNIST CNN is almost perfect for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convolutional Neural Network Estimator, built with tf.layers (originally for MNIST).\"\"\"\n",
    "\n",
    "#  FROM : https://www.tensorflow.org/tutorials/layers#building_the_cnn_mnist_classifier\n",
    "#  CODE : https://www.tensorflow.org/code/tensorflow/examples/tutorials/layers/cnn_mnist.py\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "from tensorflow.contrib.learn.python.learn.estimators import run_config\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)  # Quite a lot...\n",
    "#tf.logging.set_verbosity(tf.logging.WARN)   # This prevents Logging ...\n",
    "\n",
    "do_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print('Tensorflow:',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expecting:\n",
    "```\n",
    "Tensorflow: 1.0.0\n",
    "3.5.2 (default, Sep 14 2016, 11:28:32) \n",
    "[GCC 6.2.1 20160901 (Red Hat 6.2.1-1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "dataset = pickle.load(open(os.path.join('data', prefix+'.pkl'), 'rb'))\n",
    "\n",
    "train_indices = [ i for i,r in enumerate(dataset['rand']) if r<=0.9 ]\n",
    "check_indices = [ i for i,r in enumerate(dataset['rand']) if r>0.9 ]\n",
    "\n",
    "print(\"Training and Validation(='check_') data loaded, %d items total \" % (len(dataset['stamp']),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, integer_labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "    features_images=features['images']\n",
    "\n",
    "    input_layer = tf.reshape(features_images, [-1, 64, 32, 1], name='input_layer')\n",
    "\n",
    "    # Convolutional Layer #1 (5x5 kernels)\n",
    "    conv1 = tf.layers.conv2d( inputs=input_layer,\n",
    "      filters=16, kernel_size=[5, 5], padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 (5x5 kernels)\n",
    "    conv2 = tf.layers.conv2d( inputs=pool1,\n",
    "      filters=16, kernel_size=[5, 5], padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2 (2x2 filter and stride of 2)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    pool2_flat = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    # Dense Layer\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=32, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.5 probability that element will be kept\n",
    "    dropout = tf.layers.dropout( inputs=dense, rate=0.5, training=(mode == learn.ModeKeys.TRAIN) )\n",
    "\n",
    "    # Logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_of_classes)\n",
    "    #logits = tf.Print(logits, [input_layer.get_shape(), integer_labels.get_shape()], \n",
    "    #           \"Debug size information : \", first_n=1)\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        onehot_labels = tf.one_hot(indices=tf.cast(integer_labels, tf.int32), depth=num_of_classes)\n",
    "        loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=onehot_labels)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss( loss=loss,\n",
    "                          global_step=tf.contrib.framework.get_global_step(),\n",
    "                          learning_rate=0.001, optimizer=\"Adam\")\n",
    "\n",
    "    # Generate Predictions\n",
    "    predictions = {\n",
    "        \"classes\":       tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\"), \n",
    "        \"logits\":        logits,\n",
    "    }\n",
    "\n",
    "    # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps( mode=mode, predictions=predictions, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator : https://www.tensorflow.org/extend/estimators\n",
    "tf_random_seed = 100\n",
    "config = run_config.RunConfig(tf_random_seed=tf_random_seed)\n",
    "\n",
    "cnn_classifier = learn.Estimator(\n",
    "    model_fn=cnn_model_fn, \n",
    "    model_dir=\"cnn_model/\"+prefix, # This is relative to the ipynb\n",
    "    config=config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_input_fn(dataset, indices, batch_size=100, seed=None, num_epochs=1):  \n",
    "    # If seed is defined, this will shuffle data into batches\n",
    "\n",
    "    # Get the data into tensorflow\n",
    "    stamps = np.array( dataset['stamp'] )[indices]\n",
    "    print(\"stamps.shape:\", stamps.shape)\n",
    "    labels = np.array( dataset['label'] )[indices]\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "    \n",
    "    # Ensure that the stamps are 'float32' in [0,1] and have the channel=1\n",
    "    stamps_with_channel = np.expand_dims( stamps / 255.0, -1)\n",
    "\n",
    "    all_images = tf.constant( stamps_with_channel, shape=stamps_with_channel.shape, dtype=tf.float32 )\n",
    "    all_labels = tf.constant( labels, shape=labels.shape, verify_shape=True )\n",
    "    \n",
    "    print(\"batch_input_fn sizing : \", all_images.shape, )\n",
    "    \n",
    "    if True:  # This is if the number of examples is large enough to warrant batching...\n",
    "        # And create a 'feeder' to batch up the data appropriately...\n",
    "        image, label = tf.train.slice_input_producer( [ all_images, all_labels ], \n",
    "                                               num_epochs=num_epochs,\n",
    "                                               shuffle=(seed is not None), seed=seed,\n",
    "                                             )\n",
    "\n",
    "        dataset_dict = dict( images=image, labels=label ) # This becomes pluralized into batches by .batch()\n",
    "\n",
    "        batch_dict = tf.train.batch( dataset_dict, batch_size,\n",
    "                                    num_threads=1, capacity=batch_size*2, \n",
    "                                    enqueue_many=False, shapes=None, dynamic_pad=False, \n",
    "                                    allow_smaller_final_batch=False, \n",
    "                                    shared_name=None, name=None)\n",
    "\n",
    "        batch_labels = batch_dict.pop('labels')\n",
    "    \n",
    "    if False:\n",
    "        batch_dict = dict( images=all_images )\n",
    "        batch_labels = all_labels\n",
    "    \n",
    "    # Return : \n",
    "    # 1) a mapping of feature columns to Tensors with the corresponding feature data, and \n",
    "    # 2) the corresponding labels\n",
    "    return batch_dict, batch_labels\n",
    "\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_training:\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook( tensors=tensors_to_log, every_n_secs=20 ) #every_n_iter=1000 )\n",
    "\n",
    "    # Train the model\n",
    "    epochs=200\n",
    "\n",
    "    if False:\n",
    "        cnn_classifier.fit(\n",
    "          x=train_data,\n",
    "          y=train_labels,\n",
    "          batch_size=batch_size,\n",
    "          steps=train_labels.shape[0]/batch_size * epochs,\n",
    "          monitors=[logging_hook]\n",
    "        )\n",
    "\n",
    "    cnn_classifier.fit(\n",
    "        input_fn=lambda: batch_input_fn(dataset, train_indices, batch_size=batch_size, \n",
    "                                        seed=tf_random_seed, num_epochs=epochs), \n",
    "        \n",
    "        #input_fn=lambda: batch_input_fn(dataset, train_indices, batch_size=len(train_indices), \n",
    "        #                                seed=tf_random_seed, num_epochs=None), \n",
    "        #steps=epochs,\n",
    "        \n",
    "        #monitors=[logging_hook],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure the accuracy metric for evaluation\n",
    "cnn_metrics = {\n",
    "  \"accuracy\":\n",
    "      learn.MetricSpec(\n",
    "          metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),\n",
    "}\n",
    "\n",
    "# Evaluate the model and print results\n",
    "#cnn_eval_results = mnist_classifier.evaluate( x=eval_data, y=eval_labels, metrics=cnn_metrics)\n",
    "\n",
    "cnn_check_results = cnn_classifier.evaluate(\n",
    "    input_fn=lambda: batch_input_fn(dataset, check_indices, batch_size=len(check_indices)), \n",
    "    steps=1,\n",
    "    metrics=cnn_metrics,\n",
    ")\n",
    "\n",
    "print(cnn_check_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... comment on results ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at some 'live examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pickle.load(open(os.path.join('data', prefix+'-test.pkl'), 'rb'))\n",
    "\n",
    "print(\"Ad-hoc test data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_dataset( ds ):\n",
    "    indices = range( len(ds['stamp']) )\n",
    "\n",
    "    cnn_predictions_generator = cnn_classifier.predict( \n",
    "        input_fn=lambda: batch_input_fn(ds, indices, batch_size=1),\n",
    "        #outputs=['probabilities'],\n",
    "    )\n",
    "    \n",
    "    predictions = [p for p in cnn_predictions_generator]\n",
    "    for i,p in enumerate(predictions):\n",
    "        label = int(ds['label'][i])\n",
    "        if label>=0:\n",
    "            p['word'] = ds['words'][label]\n",
    "        else:\n",
    "            p['word'] = ds['words'][i]\n",
    "        p['label'] = label\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = get_predictions_for_dataset(dataset_test)\n",
    "\n",
    "print()\n",
    "for i, prediction in enumerate(predictions):\n",
    "    probs = ','.join([ \"%6.2f%%\" % (p*100,) for p in prediction['probabilities']] )\n",
    "    print( \"%s == %d  p=[%s]\" % (dataset_test['words'][i], prediction['classes'],  probs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heat_map(heat_map, yticks=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.imshow(heat_map, interpolation='nearest', cmap=plt.cm.Blues, aspect='auto')\n",
    "    plt.xticks( range(10) )\n",
    "    if yticks:\n",
    "        plt.yticks( range(len(heat_map)), yticks )\n",
    "    else:\n",
    "        plt.yticks( range(len(heat_map)) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a heat map...\n",
    "heat_map = [ prediction['probabilities'] for prediction in predictions]\n",
    "\n",
    "show_heat_map(heat_map) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra... \n",
    "\n",
    "What happens if we try to look at the 'animals' test with the 'num' network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_animals = pickle.load(open(os.path.join('data', 'animals.pkl'), 'rb'))\n",
    "\n",
    "predictions_animals = get_predictions_for_dataset(dataset_animals)\n",
    "\n",
    "heat_map = [ p['probabilities'] for p in predictions_animals]\n",
    "\n",
    "show_heat_map(heat_map, [ p['word'] for p in predictions_animals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = [ p['logits'] for p in predictions_animals]\n",
    "show_heat_map(heat_map, [ p['word'] for p in predictions_animals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "animal_features, animal_targets=[],[]\n",
    "for p in predictions_animals:\n",
    "    #animal_features.append( p['probabilities'] )\n",
    "    animal_features.append( p['logits'] )\n",
    "    animal_targets.append( p['label'] )\n",
    "\n",
    "animals_from_numbers_svm_classifier = svm.LinearSVC()\n",
    "animals_from_numbers_svm_classifier.fit(animal_features, animal_targets) # learn from the data (QUICK!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_animals_test = pickle.load(open(os.path.join('data', 'animals-test.pkl'), 'rb'))\n",
    "\n",
    "predictions_animals_test = get_predictions_for_dataset(dataset_animals_test)\n",
    "\n",
    "print('\\n\\nanimals class predictions from SVM classifier based on digits-CNN output')\n",
    "for i,p in enumerate(predictions_animals_test):\n",
    "    #svm_prediction = animals_from_numbers_svm_classifier.predict( p['probabilities'].reshape(1,-1) )\n",
    "    svm_prediction = animals_from_numbers_svm_classifier.predict( p['logits'].reshape(1,-1) )\n",
    "    #decision     = animals_from_numbers_svm_classifier.decision_function([ np_logits[0] ])\n",
    "    \n",
    "    print(\"Sound[%d] is '%s' - predicted class[%d] = '%s'\" % (\n",
    "            i, dataset_animals['words'][i], \n",
    "            svm_prediction, dataset_animals['words'][svm_prediction[0]],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... close, but no cigar ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
