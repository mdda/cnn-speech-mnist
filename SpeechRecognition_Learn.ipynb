{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN to Recognise Words\n",
    "\n",
    "The stamp process in the '_Data' notebook has produced some nice-looking spectrograms with a uniform (64,32) shape. \n",
    "\n",
    "Let's just recognise the words the stamps represent by learning to differentiate between the 'stamp' images : a task for which the MNIST CNN is almost perfect for..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convolutional Neural Network Estimator, built with tf.keras (originally for MNIST).\"\"\"\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "# nvidia-smi --gpu-reset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "do_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)\n",
    "print('Tensorflow:', tf.__version__)\n",
    "print('Keras:', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expecting:\n",
    "```\n",
    "3.6.6 (default, Jul 19 2018, 14:25:17) \n",
    "[GCC 8.1.1 20180712 (Red Hat 8.1.1-5)]\n",
    "Tensorflow: 1.12.0\n",
    "Keras: 2.1.6-tf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "dataset = pickle.load(open(os.path.join('data', prefix+'.pkl'), 'rb'))\n",
    "\n",
    "train_indices = [ i for i,r in enumerate(dataset['rand']) if r<=0.9 ]\n",
    "check_indices = [ i for i,r in enumerate(dataset['rand']) if r>0.9 ]\n",
    "\n",
    "print(\"Training and Validation(='check_') data loaded, %d items total \" % (len(dataset['stamp']),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "input_shape = (64, 32, 1)  # tf backend is channels_last\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset iterator\n",
    "\n",
    "def make_dataset(data, indices, seed=None, num_epochs=1, batch_size=batch_size):  \n",
    "    # If seed is defined, this will shuffle data into batches\n",
    "\n",
    "    # Get the data into tensorflow\n",
    "    stamps = np.array( data['stamp'] )[indices]\n",
    "    print(\"stamps.shape:\", stamps.shape)\n",
    "    # Ensure that the stamps are 'float32' in [0,1] and have the channel=1\n",
    "    stamps_with_channel = np.expand_dims( stamps / 255.0, -1)\n",
    "\n",
    "    labels = np.array( data['label'] )[indices]\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "    labels_one_hot = keras.utils.to_categorical(labels, num_classes)\n",
    "\n",
    "    all_images = tf.constant( stamps_with_channel, shape=stamps_with_channel.shape, dtype=tf.float32 )\n",
    "    all_labels = tf.constant( labels_one_hot, shape=labels_one_hot.shape, verify_shape=True )\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices( (all_images, all_labels) )\n",
    "    if seed is not None:\n",
    "        ds = ds.shuffle(batch_size*4)\n",
    "    \n",
    "    ds = ds.repeat(num_epochs).batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = make_dataset(dataset, train_indices, num_epochs=num_epochs, seed=100)  # shuffles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_check = make_dataset(dataset, check_indices, num_epochs=num_epochs, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe = len(train_indices) // batch_size\n",
    "spe, len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ds_train, steps_per_epoch=spe, epochs=num_epochs, \n",
    "          validation_data=ds_check, validation_steps=len(check_indices), \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(ds_check, steps=len(check_indices), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... comment on results ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at some 'live examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pickle.load(open(os.path.join('data', prefix+'-test.pkl'), 'rb'))\n",
    "\n",
    "print(\"Ad-hoc test data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_dataset( data ):\n",
    "    n_points = len(data['stamp'])\n",
    "    ds = make_dataset(data, range( n_points ), num_epochs=1, batch_size=1)\n",
    "\n",
    "    pred_arr = model.predict(ds, steps=n_points, verbose=0)\n",
    "    #print(pred_arr)  # This is an array of predictions, each with n_classes of probs\n",
    "\n",
    "    predictions = [ dict(classes=i, probabilities=p, logits=np.log(p+1e-20)) \n",
    "                    for i, p in enumerate(pred_arr) ]\n",
    "    \n",
    "    for i, p in enumerate(predictions):\n",
    "        label = int(data['label'][i])\n",
    "        if label>=0:\n",
    "            p['word'] = data['words'][label]\n",
    "        else:\n",
    "            p['word'] = data['words'][i]\n",
    "        p['label'] = label\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = get_predictions_for_dataset(dataset_test)\n",
    "\n",
    "print()\n",
    "for i, prediction in enumerate(predictions):\n",
    "    probs = ','.join([ \"%6.2f%%\" % (p*100,) for p in prediction['probabilities']] )\n",
    "    print( \"%s == %d  p=[%s]\" % (dataset_test['words'][i], prediction['classes'],  probs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heat_map(heat_map, yticks=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.imshow(heat_map, interpolation='nearest', cmap=plt.cm.Blues, aspect='auto')\n",
    "    plt.xticks( range(10) )\n",
    "    if yticks:\n",
    "        plt.yticks( range(len(heat_map)), yticks )\n",
    "    else:\n",
    "        plt.yticks( range(len(heat_map)) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a heat map...\n",
    "heat_map = [ prediction['probabilities'] for prediction in predictions]\n",
    "\n",
    "show_heat_map(heat_map) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra... \n",
    "\n",
    "What happens if we try to look at the 'animals' test with the 'num' network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_animals = pickle.load(open(os.path.join('data', 'animals.pkl'), 'rb'))\n",
    "\n",
    "predictions_animals = get_predictions_for_dataset(dataset_animals)\n",
    "\n",
    "heat_map = [ p['probabilities'] for p in predictions_animals]\n",
    "\n",
    "show_heat_map(heat_map, [ p['word'] for p in predictions_animals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = [ p['logits'] for p in predictions_animals]\n",
    "show_heat_map(heat_map, [ p['word'] for p in predictions_animals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "animal_features, animal_targets=[],[]\n",
    "for p in predictions_animals:\n",
    "    #animal_features.append( p['probabilities'] )\n",
    "    animal_features.append( p['logits'] )\n",
    "    animal_targets.append( p['label'] )\n",
    "\n",
    "animals_from_numbers_svm_classifier = svm.LinearSVC()\n",
    "animals_from_numbers_svm_classifier.fit(animal_features, animal_targets) # learn from the data (QUICK!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_animals_test = pickle.load(open(os.path.join('data', 'animals-test.pkl'), 'rb'))\n",
    "\n",
    "predictions_animals_test = get_predictions_for_dataset(dataset_animals_test)\n",
    "\n",
    "print('\\n\\nanimals class predictions from SVM classifier based on digits-CNN output')\n",
    "for i,p in enumerate(predictions_animals_test):\n",
    "    #svm_prediction = animals_from_numbers_svm_classifier.predict( p['probabilities'].reshape(1,-1) )\n",
    "    svm_prediction = animals_from_numbers_svm_classifier.predict( p['logits'].reshape(1,-1) )\n",
    "    #decision     = animals_from_numbers_svm_classifier.decision_function([ np_logits[0] ])\n",
    "    \n",
    "    print(\"Sound[%d] is '%s' - predicted class[%d] = '%s'\" % (\n",
    "            i, dataset_animals['words'][i], \n",
    "            svm_prediction, dataset_animals['words'][svm_prediction[0]],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Any good? ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
